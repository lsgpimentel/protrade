\documentclass[10pt]{article}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{url}

% global commands

\newcommand{\javalst}[2]{
  \lstset{language=Java,captionpos=b,tabsize=4,frame=single,numbers=left,
    numberstyle=\tiny,numbersep=10pt,breaklines=true,showstringspaces=false,
    basicstyle=\footnotesize,emph={label}, caption={#1}, label={#2}}
}
\newcommand{\initlisting}[2]{
  \lstset{language={#1},captionpos=b,tabsize=4,frame=single,numbers=left,
    numberstyle=\tiny,numbersep=10pt,breaklines=true,showstringspaces=false,
    basicstyle=\footnotesize,emph={label}, caption={#2}}
}

\newcommand{\nm}{{\bf proTrade}}
\newcommand{\nmsp}{{\nm \ }}
% end commands
\setlength{\parskip}{0.3cm}
\setlength{\parindent}{0cm}

\begin{document}

\title{Report Three: Software Validation}

\author{Corina Ciobanu \and Iskander Orazbekov \and Mir Sahin \and Paul Grigoras \and Radu Baltean-Lugojan}

\date{\today}         % inserts today's date

\maketitle            % generates the title from the data above

\clearpage

\begin{abstract}
  \nmsp is a tennis trading environment which delivers the information a trader requires to place bets. Betting functionality, linked to the user's bank account is also provided. Testing and validation are crucial in order to deliver a reliable and secure application and have been used extensively.
\end{abstract}

\clearpage

\tableofcontents

\clearpage

\section{Introduction}

Tennis trading is a steadily growing market on the Betfair Exchange, with more
than 70\% of bets being placed in-play. In order to maintain market liquidity,
exchanges must attract customers. One way to achieve this is to provide the
users with better tools which, by providing more information and better visual-
ization techniques, can help the trader improve his understanding and predict
the market evolution, ultimately leading to an increase in his profit.

In the case of tennis trading, the information required to understand and predict
the evolution of the market associated with a particular match consists of the
live score, player statistics, potentially a live video feed and - of course - the
market data (evolution of betting odds). Ideally, this information is desired for
both historical and live matches.

At the moment, no application provides all this information. A number of
solutions exist which allow visualization of historical market data, but they
generally lack the more specific, tennis related data. For example the Fracsoft
Data Viewer (\cite{site-fracsoft}) does not correlate market data with match data (scores, player statistics). BetAngel (\cite{site-betangel}) provides some tennis related data and prediction, but relies on the user to input the current match score by pushing buttons. This is
not entirely suited for the high rate at which data is updated. Ideally, all the information should be automatically provided to the user to increase update speed and enhance usability.

\nmsp means to fill this gap by providing all the information along with
betting and prediction functionalities for both historical and live matches, in an
entirely automated fashion.

Due to the high risk associated with some user actions (e.g. placing bets with
real money) and in order to ensure reliability and stability, testing and other
validation methods have been used extensively. These are covered in detail in
section \ref{sec-testing} below.

In addition the challenging nature of the project brought teamwork and manage-
ment at the heart of a development process that aimed to deliver core function-
alities and extensions on time. The aspects of group management are discussed
in section \ref{sec-group}.

\clearpage

\section{Testing}
\label{sec-testing}

We started to use testing as soon as the first cohesive draft of the Match API, containing all core features, was created (iteration 3). This allowed us to validate our design and do any major refactoring work around that stage. From the beginning of iteration 5, testing has been supplemented with continuous integration and measuring code coverage to ensure a continuous monitoring of the state of the project. After obtaining all necessary data for building the Prediction API, further tests were developed along the way, leading to a TDD approach for the latter part of the project.

As suggested in \cite{bk-testing} we have used the tests to guide our design, based on the principle that if a project is well designed it should be easy to write meaningful tests for it.

\textbf{Unit testing} has been used to rapidly test small portions of code while \textbf{acceptance tests} have been used to test a system feature from front-end to back-end. \textbf {TDD tests} (tests written before features) are expected to fail when they are written and should pass once the feature has been completely implemented. Once it passes, a test is transferred to the regression suite. Naturally, a failing \textbf{regression test} indicates a break in previous functionality (regression).

The general approach was to use TDD for the model packages, based on the fact that these should be easier to test, are crucial for the application and less likely to change.

Because of the lack of familiarity with the UI libraries and the limitations and lack of documentation of the UI testing framework - which could have threatened the ability to deliver the project on time - we avoided a TDD approach for the UI classes, but resorted to writing post implementation acceptance tests to cover a decent proportion (generally over 60-70\%) of the user interface functionality.

\subsection{Unit Testing}
Unit testing was adopted early on in the development process and it has been of great use in identifying bugs and ensuring correctness of the Match API (an important set of classes which manages data such as match score and statistics). 

One specific use case was to test the implementation of the scoring rules. Since, in tennis, these can be quite peculiar (for example points are counted 15, 30, 40, AD instead of just 1, 2, 3, 4) and tests are particularly easy to write, we adopted a TDD approach in this instance. A simple initial test was written to state the expected outcome of adding four consecutive points to one of the players: the player in cause should win the game, increasing his game score, and resetting the points score. Furthermore the opponent's score should always be zero. The code for this is presented in Listing \ref{list1}.

\javalst{Initial failing test for score. The initialisation of the score object is handled in an abstract super class which also provides assertSetScoreIs() and assertGamesScoreIs().}{list1}
\begin{lstlisting}
  @Test
  public void fortyZeroWin(){
    int expectedPoints[] = {15,30,40};
    for (int i=0;i<3;i++){
      score.addPlayerTwoPoint();
      assertGameScoreIs(0,0);
      assertPointsScoreIs(0, expectedPoints[i]);
    }
    score.addPlayerTwoPoint();
    assertGameScoreIs(0,1);
    assertPointsScoreIs(0,0);
  }
\end{lstlisting}

Having first written the test and ensured it failed, we then proceeded to designing and implementing a solution that would make it pass. We adopted this approach for the whole Match API and for the Prediction API (used to predict the evolution of a match based on current score and player statistics).

This approach helped us identify numerous bugs early on and guided us towards a better overall design of the APIs. For example, a function for correctly setting a set score to a certain value was not initially provided, but since while writing the tests the need for such a function became obvious, it was included and tested.

\subsection{Acceptance Testing}

We used acceptance tests to test a particular function of the system, starting
from the front-end (e.g. finding and pushing a button on the UI) to the back-end
(e.g. connecting to the Betfair API to authorize a login request).

For performance reasons these were ran separately from unit tests since the UI
operations tend to be slow.

A simple example is the test in Listing \ref{list2} which checks the login functionality:
the user should fill in their Betfair account and password and click on the login
button. The login attempt is checked against the Betfair API and a label is
updated to indicate success or failure. Obviously, an attempt to login with the
specially created test account should result in a success message being displayed.

We have adopted a similar approach in testing most of the classes connected to
the user interface.

\clearpage

\javalst{Initial failing test for the login window. The username and password for the test account are read and decrypted from a local config file by the Main class. Using the UI bot we then fill the data in on the login window and click the login button. Similar tests have been written to verify other scenarios.}{list2}
\begin{lstlisting}
    @Test
    public void correctLoginSuccess() throws Exception {
        SWTBotText username = bot.text("username");
        username.setText(Main.getTestUsername());
        SWTBotText password = bot.text("password");
        password.setText(Main.getTestPassword());
        loginButton.click();
        SWTBotLabel success = bot.label(LoginShell.SUCCESS);
        assertNotNull(success);
    }
\end{lstlisting}

However, due to limitations in the SWTBot API some features have proven impossible to test. For example we have not found a way to test the functions of a context menu (pop up) or a progress bar.
Nevertheless these are important aspects which must be tested to ensure correct behaviour. For example, one of the most important tools for traders is the chart on which they can view the market's evolution. Since a context menu on the chart allows the user to choose between different types of analysis, it is currently impossible to completely test the correctness of this function.

In such situations we have resorted to testing the actions executed by the specific listener associated with the context menu item, which at the very least ensures that if the event fires correctly, the display is updated as expected.

\subsection{Regression Testing}
We have used regression testing to ensure that no break in previous functionality occurs when introducing new code.

Keeping tests that measure progress separate from regression tests is key in quickly identifying when a regression has occured. Simply put, all tests appart from those in the progress suite should pass at all times. If this is not the case, a regression has occurred.

Consequently the normal development flow is: 
\begin{enumerate}
\item write a failing test (and ensure it fails)
\item make the test pass
\item check for regressions
\item fix any regression
\item move test from progress suite to regression suite
\item commit changes
\item repeat
\end{enumerate}

\subsection{Integration Testing}

Given the dependency on a large number of external APIs, we have used integration testing to ensure that these behave as expected and quickly identify potential configuration issues. 

This has been done by writing tests that ensure the abstractions built on top of the libraries work as expected. To reduce coupling (and simplify the testing process), normally one interface and an implementation class expose the required functionality, encapsulating all other logic required for manipulating the library. As examples we discuss the testing approach for three of the most heavily used libraries: the Betfair API, SWT and SWTChart.

\subsubsection{Betfair API}

The Betfair API is organised in three services: global, UK Exchange and AUS Exchange, each defined through a WSDL specification. The application only uses the first two services, accessing them through Java classes automatically generated with Apache Axis.

Any connection to Betfair and use of the API is handled by classes in the src.model.connection package. Moreover, to ensure both services (Global and UK Exchange) only have one point of access, all the functionality required by the application has been encapsulated in two classes - BetfairExchangeHandler and BetfairConnectionHandler. Hence, tests written for these classes ensure the connection to the Betfair API works as expected and constitute integration tests.

\subsubsection{SWT}

SWT is a graphical widget toolkit with a platform dependent implementation. At the heart of its architecure is the ability to create native widgets. However this raises the issue of freeing allocated memory (which cannot be garbage collected by the JVM).
Hence, it is important to test classes using this library(most of our UI classes) to ensure that the UI behaves similarly on different platforms and that there are no memory leaks.

However, precisely matching UIs for different platforms and reliably identifying memory leaks have proven to be very difficult tasks, so we have postponed handling of these aspects to later stages.

In all other respects, we have considered the acceptance tests as good enough to test integration with the SWT library.

\subsubsection{SWTChart}

The SWTChart API provides a chart component with several basic functionalities (such as drawing line functions, bar charts etc). The chart can handle real-time updates (even for large data series), which is crucial to the purpose of our project and, since it is based on SWT, it integrates smoothly with the application's UI design and implementation. 

The addition of new functionalities requires extending the Chart class. This design encapsulates the use of the SWTChart API, so integration tests for this class ensure the library integrates well with the rest of the application. 

\subsection{Continuous Integration}

Starting with iteration five, the continuous integration server became a key element in monitoring project evolution with a view towards understanding and adapting to development trends.

\subsubsection{Configuration}
A continuous integration server has been installed on a virtual machine provided by the Computing Support Group, which emulates a dual core, 1GB memory, 64bit machine, running Ubuntu 11 (these details are important due to the platform dependent nature of the SWT library).

We have used Hudson since - unlike other CI solutions - it comes with a plugin for running UI tests on a headless server. This was an important requirement since, in order to run correctly, all acceptance tests must connect to a display instance. (Another alternative would have been to use xvfb -X virtual frame buffer - which buffers display data in memory and works for any application).

Additionally, Hudson presented a few other advantages such as smaller installation size, good out-of-the-box support for git repositories and an easily configurable build management process.

The CI server is set to poll the repository and, when changes are detected, checks out a fresh copy and runs the normal build script. As an extension, since the VM is not accessible from outside the local site, a step to move reports and artefacts to a publicly accessible location (group directory / home directory) has been added in order to simplify the process of checking the build status.

The build process generates reports for unit tests, system tests, code quality (discussed in section \ref{sec-quality}) and test coverage as well as historical build trends (e.g. duration).

\subsubsection{Reporting}
\label{sec-reporting}
The reports generated by the CI server were important in understanding the historical evolution of the project. For example, the build duration statistics allowed members to easily understand if the ``ten minute build'' rule is enforced.

Monitoring build ``colour'' revealed the need for a more established code review system. For example, the points where the build is red (45 - 49, 53 - 55, 63 - 66) coincide with periods slightly before supervisor meetings or other presentations and reveal that developers tend to break the build in such crucial moments, most likely, as a result of their desire to contribute last minute additions. Fortunately, the team was able to fix these issues in time but decided nevertheless that such situations must be avoided. As a result, a more rigorous code review mechanism was adopted (presented in Section \ref{sec-codechange}).

\begin{figure}[ht]
\centering
\includegraphics[bb=0 0 500 400, scale = 0.5]{build-trend.png}
\caption{Build trend summary. Vast majority of builds are well under 10 minutes. Outlying values were caused by a network outage.}
\end{figure}

\subsection{Measuring Test Coverage}

Since we did not adopt a TDD approach from the very beginning, it was important to obtain an overview of the parts of the code that required testing. Starting with iteration 5 we have used Cobertura,  an open source tool which provides neat test coverage reports for Java programs.

To facilitate report generation, a single ant task has been set up to compile the code with the debug info (vital for Cobertura to indicate line numbers and measure coverage), run all the tests (unit, acceptance, integration) and generate human readable reports which provide an indication of the current test coverage as well as branch coverage and complexity measures.

The coverage reports enabled us to identify lines which were not touched by tests. This was usually fixed by writting another test to cover the specific path, but it could also have been the case that the functionality was actually never required, in which case it was completely removed. Again this illustrates how tests have been used to ensure we are running a healthy codebase.

\begin{figure}[ht]
\centering
\includegraphics[bb=0 0 1680 1050, scale = 0.2]{coverage.png}
\caption{Coverage report generated by Cobertura indicates test coverage at package level, branch coverage an cyclomtic complexity.}
\end{figure}

Branch coverage indicates when tests do not cover particular cases and has proven useful with regards to particularly tricky conditionals.

Cobertura also generates cyclomatic complexity values for each class/package, measuring the number of independent paths through the control flow graph of the code. Since it has been shown that high values are usually an indication of error prone code ([2]) this measurement is used to check code sanity.

\begin{figure}[ht]
\centering
\includegraphics[bb=0 0 700 100, scale = 0.49]{branch.png}
\caption{Branch coverage indicates specific condition coverage.}
\end{figure}

\subsection{Code Quality Measures}
\label{sec-quality}
We have used PMD to generate code quality reports. Additionally we have researched and are looking forward to using FindBugs - a tool which identifies bugs in Java code (such as NPEs and potential out of bounds accesses).

\begin{figure}[ht]
\centering
\includegraphics[bb=0 0 1680 1050, scale = 0.2]{pmd.png}
\caption{PMD reports indicate potential quality issues. The tool can be configured to signal a large number of potential issues and comes with predefined rules for optimizations, unit tests, unused code etc.}
\end{figure}

\subsection{Testing Overview}

All core components of the application follow the MVC pattern and this is reflected in the structure of the tests, with unit tests validating the back-end (Model) and functional/acceptance tests verifying the view and controller layers (incidentally acceptance tests also touch model functionality, but their overal scope is broader). Additionally, integration tests are used to reproduce and analyse behaviour of specific libraries. A functional overview of the application’s code base and features together with their tests is given below for subpackages of org.ic.tennistrader.

\begin{figure}[ht]
\begin{center}$
\begin{array}{ccc}
\includegraphics[bb=0 0 499 700, scale = 0.25]{project.png} &
\includegraphics[bb=0 0 499 700, scale = 0.25]{tests-unit.png} &
\includegraphics[bb=0 0 499 700, scale = 0.25]{tests-system.png} 
\end{array}$
\end{center}
\caption{Left - Project hierarchy with included packages. Center - Unit tests for local functionality/model based classes/packages. Right - System tests for controller/UI oriented classes/packages}
\end{figure}

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
\item .authentication: Encrypt/decrypt functions for storing and accessing the login credentials are unit tested, while the interface (LoginShell) is acceptance tested (as detailed in section 2.2).
\item .controller: The bet controller, built upon relevant back/lay buttons and their associated bet placing functionality, is tested for correct bet types and player recognition.
\item .domain: Local functionality of populating/retrieving the chart widget with market data is unit tested in 'ChartDataTest'. Specific tests for other domain classes have not been designed, since these are plain java objects, with no logic implementation. However, their functions are tested when these objects are used in other unit-tests.
\item .domain.match: Special unit-tests have been designed to check how the score of a match is updated and handled.
\item .model and .model.connection: The implementation and logic of virtual betting (including the internal state representation of the market and matched/unmatched bets) are unit tested, while the global and exchange connections to Betfair API (that normally update the internal state) are validated via integration tests.     
\item .score: Front end 'PredictionGui' controller is functionally tested, whereas specific unit tests are written for back-end components such as statistics fetching and parsing or the 'PredictionCalculator' model. 
\item .service: Since this package models the back end receiver for various incoming data, state updating is unit tested.
\item .ui and .ui.betting: Acceptance and testing of all front-end GUI interfaces.
\item .ui.updatable: Acceptance and integration testing on dynamical GUI elements driven by user input or data updates.
\end{itemize}

\subsection{Bugs Revealed by Testing}

Apart from validating design, testing has revealed hidden bugs in various components of the system at all tiers of the application. Below we summarise a few examples.

Unit tests have invalidated a first encoding of the scoring system for the prediction calculator, when translating between traditional tennis point scores and an incremental representation.

Also, they have identified a missing condition when registering events whose market data should constantly updated by connecting to the Betfair API, which resulted in some events being introduced twice.

Furthermore, they have revealed some overlooked considerations in recursive state passing when computing a set prediction based on a Markov chain model, which led to wrong set winning probability estimates. 

\clearpage

\section{General Validation}

We have used validation methods to ensure that the product accomplishes its intended requirements.

\subsection{User interface design validation}
The user interface of our application has been designed with a view towards
obtaining a final product which achieves the following goals: being visually
eye-pleasing and simple to use, and at the same time minimizing the effort it
takes for users to accomplish their goals. To the purpose of validating our user
interface design, feedback has been collected from a number of different persons
with or without experience in using trading applications.

\subsubsection{Feedback from Experienced Users}

Firstly, we have had constant feedback from our supervisor, who is familiar with the game of tennis and the world of tennis betting and who understands the needs of a professional trader. He has guided us through the process and has pointed out a number of possible UI improvements along the way (such as positioning different components, displaying different data on the graph), which we have accomplished.
 
Secondly, we have had meetings with PhD students, who are themselves developing trading applications. They provided valuable feedback regarding market information display and the best way to present it to a specialized user. We have collected a number of user satisfaction questionnaires to guide our understanding of the relative importance of such features (see next page for an example). Hence, our initial interface has been adapted to accommodate their suggestions.

Finally, to collect feedback from people in the industry, our supervisor arranged a meeting the Head of Research at Betfair. He has approved of our application’s general interface and pointed out a few possible issues (such as synchronizing all the playback data), which are now fixed.

\subsubsection{Feedback from Unexperienced users}

To test how easy it is for users to understand the information presented by the application and how ``natural'' it is to find the required functionalities, we have collected feedback from a few colleagues, without previous experience with tennis trading applications and the feedback received was generally positive.

\clearpage
(Left blank for sample user questionnaire)
\clearpage


\subsection{Code Review}

We also used a review mechanism in which, generally, code has been inspected by at least two team members.

During the implementation process we often employed pair programming but also other lightweight code review mechanisms, particularly over-the-shoulder and tool-assisted code review (e.g. based on PMD suggestions). The decision to adopt lightweight code review mechanisms was based on evidence which suggests that this type of review can be as effective as a more formal one, but easier to implement.

This helped improve the overall quality of the software and minimize the number of bugs early on in the development process. The whole source code is accessible to every member of the group and where appropriate and necessary, the code is reviewed by other members who don’t work on that specific widget or feature. 

Wherever a bug is found or refactoring is needed, an issue is raised on GitHub with the suggestion to solve that specific problem.

\subsection{Stress testing}

At the moment, no stress testing tools are used. This has been left as the last stage of our development process, as it may be more beneficial once we have all the features implemented. However we have conducted a ``manual'' stress  testing.

An abnormal condition or a stressing condition for our application is when a user tries to view and trade several matches/markets at the same time. This can be done by opening more than one tab, since each tab displays one match and its associated market information.

We have observed that when viewing data for only one match the application’s physical memory consumption is around 1.5 GB, and this can increase up to 2GB (with 3 open tabs). We understand such behaviour is a major problem, as an average computer has 3-4 GB of physical memory, and we aim to solve the problem by the end of the project. Hence, this has become an important issue, starting an on-going process to identify and eliminate memory leaks.

Using the Standard Widget Toolkit (SWT) to develop the graphical user interface can be a source of memory leaks, since the native OS widgets created by it cannot be garbage collected. Therefore, special attention is paid to disposing any created components.

Also, the application requires a lot of data to be stored and presented to the user in different ways. Hence, one of the aims is to have an efficient memory management, as this will avoid having duplicate data in memory.

The application does not require a lot of CPU power as most computations are negligible. The most computationally intensive tasks are the statistical data inference algorithms, but we havent observed a heavy CPU usage even with more match views open at the same time.

\clearpage

\section{Managerial Documentation}
\label{sec-group}

This section provides a formal account of group management and activities.

\subsection{Collaboration Tools}

Throughout the project various collaboration tools were used to facilitate cooperation between members, ensuring a successful product delivery.

\subsubsection{Git}

Use of git significantly improved the quality of work produced by the team, as it provided not only a shared code base with version control, but also a good picture of the current state of the project. Moreover, it encouraged team members to use ``Commit Early, Commit Often'' paradigm, as many postponed commits tend to result in a large number of merge conflicts.

\subsubsection{Github}

For the purpose of efficient collaboration, a Github repository was created, pro-
viding the team with a wide range of features.

First of all, the team used issues that could be tracked and assigned to spe-
cific group members. These had comment threads linked to them, simplifying
communication and giving an ability to have almost instant feedback on the
solution. Moreover, use of issues increased the working efficiency of the team,
for example, whenever there was a new issue added to the back log, all group
members were notified, and once one of them had finished working on their
current issue, they could move on to the next.

Additionally, Github provided the ability to split issues into different milestones,
consequently providing a better sense of direction for everyone in the group
which resulted in better, more efficient cooperation between team members.

\subsubsection{Google Docs}

Another set of collaboration tools that proved to be useful was Google Docs.
This platform allowed the whole group to exchange information by sharing doc-
uments. The team primarily used it to write meeting agendas, keep logs of the
work done via spreadsheets and set global objectives for particular iterations.

The use of Google Docs also simplified report editing by supporting collabora-
tive editing and instant feedback (via the ``comment'' feature).


\subsubsection{Dropbox}
In order to share larger files, team members created dropbox accounts which
allowed them to share data related to tennis, sports trading, Betfair API and
particular software libraries.

\subsubsection{Email, Skype and IM}

To establish a communication channel among the group members, it was es-
sential to have a good medium that everyone would have access to. Therefore,
we chose to use electronic mail and messaging services. Emails provided good
updates for the whole team and simplified the process of making proposals and
raising issues that needed consideration of all team members. Skype and IM
services, in turn, made it easier for group members to work concurrently on
specific issues and provided an excellent medium for knowledge exchange.

\subsection{Code Change Policy}
\label{sec-codechange}
In order to deliver a successful project, the team had to adopt a strict set
of policies regarding code change. The continuous reports generated by the
integration server were used to develop these policies.

For example, based on observations presented in Section \ref{sec-reporting}, the team decided to shift to a more rigorous code review mechanism: the code has been forked
into two branches: master (for development) and stable (for the latest stable
release). Only code that has been reviewed by at least two members can be
moved to the stable branch. Although this does not agree perfectly with the
``Single code base'' paradigm we believe it is a reliable approach which still
allows the team to demonstrate bleeding-edge features, with the option of easily
reverting to a more stable version should this be required.

\subsection{Knowledge Transfer}
Knowledge transfer within the group was managed using several techniques specific to the agile software development paradigm. The team made use of issues with comment threads to describe the solution. Furthermore, stand-up meetings were arranged during which more experienced members could help others overcome encountered obstacles. Finally, documents shared through Dropbox and Google Docs provided additional background information, aiming to
help members in furthering their understanding of the specific topic.

Eventually, efficient management of knowledge transfer within the team required
a constant stream of feedback on the work produced. This significantly increased
the transfer of tacit knowledge within the group, consequently helping to over-
come many obstacles in the development process.

\subsection{Materials}

Below we present the logbook and the meeting schedule for the entire duration
of the project.

\clearpage
(Intentionally left blank)

\clearpage
(Intentionally left blank)

\clearpage
(Intentionally left blank)

\clearpage
(Intentionally left blank)

\clearpage

\section{Conclusions}

In this report we have analysed the use of testing during the development process
of proTrade, indicating types of testing used, portions of code that were covered
in this manner and bugs that were revealed. We have looked at how testing has
helped improve the overall design and identify bugs early on in the development
process. We have explained how continuous monitoring of the code base (in the
form of reports generated by the integration server) has lead to improvements
in team dynamics.

Finally, we showed how collaboration tools, knowledge transfer and management
policies were placed at the heart of a team effort meant to deliver an innovative
application.

\clearpage

\begin{thebibliography}{9}
\bibitem{bk-testing}
  Steeve Freeman, Nat Pryce,
  \emph{Growing Object-Oriented Software, Guided by Tests}, Addison-Wesley 2010

\bibitem{web-cyccom}
  Robert Chatley's course on Software Engineering Methods
  \url{http://en.wikipedia.org/wiki/Cyclomatic_complexity},
  Imperial College London

\bibitem{bk-aglsam}
  Jonathan Rasmusson,
  \emph{The Agile Samurai},
  Pragmatic Bookshelf,
  October 2010.

\bibitem{bk-aglflh}
  Jeff Langr and Tim Ottinger,
  \emph{Agile in a Flash},
  Pragmatic Bookshelf, 
  January 2011.

\bibitem{web-rbc}
  Robert Chatley's course on Software Engineering Methods
  \url{http://www.doc.ic.ac.uk/~rbc/302/},
  Imperial College London

\bibitem{site-fracsoft}
  Site of Fracsoft, authors of Fracsoft Data Viewer
  \url{http://www.fracsoft.com}

\bibitem{site-betangel}
  Site of BetAngel
  \url{http://www.betangel.com}
  
\bibitem{wiki-swt}
  SWT - Wikipedia
  \url{http://en.wikipedia.org/wiki/Standard_Widget_Toolkit}

\bibitem{site-if4it}
  Site of IF4IT, list of Software Development definitions
  \url{http://www.if4it.com/SYNTHESIZED/GLOSSARY/S/Software_Development_Management_Policy.html}

\bibitem{site-pmd}
  Site of PMD
  \url{http://pmd.sourceforge.net/}

\bibitem{wiki-valid}
  Validation - Wikipedia
  \url{http://en.wikipedia.org/wiki/Verification_and_Validation}

\end{thebibliography}

\end{document}
